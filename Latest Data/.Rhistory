# See the snapshot of Demand Curves
bss_pivot <- bss %>% filter(salesdate >= "2022-01-01" & salesdate <= "2022-07-31") %>% dplyr::select(sku, price, volume, revenue, prod_cat) %>% distinct()
# product_name <- c("File Folders SKU 47","File Folders SKU 20")
product_name <- c("File Folders SKU 47")
bss_pivot %>% filter(sku %in% product_name) %>%
ggplot(aes(x = price, y = volume, color = sku)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
labs(title = "Price vs Volume for Selected Product",
x = "Price",
y = "Volume",
color = "Product") +
theme_minimal()
product_name <- c("File Folders SKU 47")
bss_pivot %>% filter(sku %in% product_name) %>%
ggplot(aes(x = price, y = revenue, color = sku)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
labs(title = "Price vs Revenue for Selected Product",
x = "Price",
y = "Revenue",
color = "Product") +
theme_minimal()
# product_category <- c("File Folders", "Classification Folders")
product_category <- c("File Folders")
bss_pivot %>% filter(prod_cat %in% product_category) %>%
ggplot(aes(x = price, y = volume, color = prod_cat)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
labs(title = "Price vs Volume for Folders",
x = "Price",
y = "Volume",
color = "Product") +
theme_minimal()
# Filter for data in 2022
recent_profit <- bss %>%
filter(year(salesdate) == 2022) %>%
# Group by quarter and summarize the profit
group_by(Qtr = quarter) %>%
summarise(Total_Profit = sum(profit, na.rm = TRUE)) %>%
ungroup()
# Convert the data to a time series object
profit_ts <- ts(recent_profit$Total_Profit, start = c(2022,1), frequency = 4)
# Plot
autoplot(profit_ts) + labs(y ="Profit Earned", title = "Quarterly Profits for 2022")
# Get the Quarterly sales for 2022
recent_sales <- bss %>%
filter(year(salesdate) == 2022) %>%
# Group by quarter and summarize the sales
group_by(Qtr = quarter) %>%
summarise(Total_Sales = sum(volume, na.rm = TRUE)) %>%
ungroup()
sales_ts <- ts(recent_sales$Total_Sales, start = c(2022,1), frequency = 4)
autoplot(sales_ts) + labs(y ="Volume Sold", title = "Quarterly Sales for 2022")
# Get the monthly sales trend for 2022
monthly_sales <- bss %>%
filter(year(salesdate) == 2022) %>%
# Group by quarter and summarize the sales
group_by(Month = month) %>%
summarise(Total_Sales_m = sum(volume, na.rm = TRUE)) %>%
ungroup()
monthly_sales_ts <- ts(monthly_sales$Total_Sales_m, start = c(2022,1), frequency = 12)
autoplot(monthly_sales_ts) + labs(y ="Volume Sold", title = "Monthly Sales for 2022")
glimpse(bss)
seed <- 123
set.seed(seed = seed) # Set seed for reproducibility
# Function to calculate the Mean Absolute Percentage Error
mape <- function(actual,forecast){
valid_rows <- which(actual != 0 & !is.na(actual) & !is.na(forecast))
actual <- actual[valid_rows]
forecast <- forecast[valid_rows]
n <- length(actual)
if(n==0){
return (NA)
}
val <- sum(abs((actual - forecast)/actual))/n * 100
return(round(val,2))
}
# Function to round the price to $0.05 or $0.09
round_to_constraint <- function(x){
integer_part <- floor(x)
decimal_part <- x - integer_part
if(decimal_part < 0.05){
return(integer_part + 0.05)
}
else if(decimal_part < 0.09){
return(integer_part + 0.09)
}
# else if(decimal_part == 0.05 || decimal_part == 0.09){
#   return(integer_part)
# }
# else {
#   return(integer_part)
# }
else{
return(integer_part + 0.05 + 1) # Best way to constraint for the decimal part > 0.09
# Basically, we're rounding it off to the next dollar for a nickel increment
}
}
bss_train <- bss %>% filter(salesdate <= "2023-05-31") # Create the training set
bss_val <- bss %>% filter(salesdate >= "2023-06-01" & sku == "File Folders SKU 47") # Create the validation set
glimpse(bss_train)
glimpse(bss_val)
######################## Hierarchical Modeling ##########################################
bss_hierachical_cost_product_intercept <- lmer(volume ~ 1 + comp_1_price + comp_2_price + cost + I(price - cost) + salesdate + weekday + (1 + cost | sku), data = bss_train, REML = TRUE)
ranef(bss_hierachical_cost_product_intercept)
plot(ranef(bss_hierachical_cost_product_intercept))
summary(bss_hierachical_cost_product_intercept)
predictions <- predict(bss_hierachical_cost_product_intercept, newdata = bss_val) # Predict the volume sold
bss_val$pred_vol <- round(predictions,0)
# stargazer(bss_hierachical_cost_product_intercept, type = "text", title = "Random Effect Results", summary = TRUE)
# Generate a Profit function to optimize the profit for the predicted volume
profit_function <- function(price, model, df) {
predicted_volume <- predict(model, newdata = transform(df, price = price)) # Predict the volume of sales for the given price by replacing the existing prices with the optimum prices using the transform() function
profit <- price * predicted_volume - df$cost # A row-wise operation to calculate the max profit for each day
return(-sum(profit, na.rm = TRUE)) #*
#* Use the negative sign to maximize for the profit as optim() by default
#  minimizes the function value.
}
# Optimizer Function
initial_guess <- median(bss_val$price, na.rm = TRUE) # Initial value for optimizing
# Set the bounds for the optimizer function
lower_limit <- min(bss_val$price, na.rm = TRUE)
upper_limit <- max(bss_val$price, na.rm = TRUE)
result <- optim(
par = initial_guess,
fn = profit_function,
model = bss_hierachical_cost_product_intercept,
df = bss_val,
method = "L-BFGS-B", # A memory efficient high-dimensional optimization method that allows the set up of requisite constraint boundaries.
lower = lower_limit,
upper = upper_limit
)
t <- round_to_constraint(result$par) # Obtain the optimal price
print(paste0("Optimal Price: ", "$",t))
########################## SIMULATED TEST SET ###############################################
temp <- bss_val %>% filter(salesdate > "2023-08-31" & salesdate <= "2023-09-11")
# Remove the last two rows
if (nrow(temp) > 2) {
temp <- temp[-((nrow(temp) - 1):(nrow(temp))), ]
} else {
# Handle the case where you have two or fewer rows
warning("Data frame has two or fewer rows. Cannot remove last two rows.")
}
temp$salesdate <- seq(from = as.Date("2023-09-17"), to = as.Date("2023-09-25"), by = "day")
temp$weekday <- wday(temp$salesdate, label = TRUE) # Introduce a weekday instrumental variable
temp$price <- t
temp$pred_vol <- round(predict(bss_hierachical_cost_product_intercept, newdata = temp, re.form = NA),0)
temp$pred_profit <- (temp$price * temp$pred_vol) - temp$cost
vv <- mape(bss_val$volume, bss_val$pred_vol)
print(paste0("Volume MAPE_Validation: ", vv, "%"))
vt <- mape(temp$volume, temp$pred_vol)
print(paste0("Volume MAPE_Test: ", vt, "%"))
p <- mape(temp$profit, temp$pred_profit)
print(paste0("Profit MAPE: ", p, "%"))
############################## Linear Regression ################################################
# Run a linear regression with volume as the response variable and the scaled prices as the features
# features <- grep("_scaled_m", colnames(bss_train), value = TRUE)
# formula <- paste("volume ~", paste(features, collapse = "+"))
# bss_lm <- lm(as.formula(formula),data = bss_train)
# summary(bss_lm)
# vif(bss_lm)
# coef(bss_lm)
# confint(bss_lm)
# residuals <- data.frame("Residuals" = bss_lm$residuals)
# res_hist <- ggplot(residuals, aes(x=Residuals)) + geom_histogram(color='black', fill='red') + ggtitle('Histogram of Residuals')
# res_hist
# par(mfrow = c(2,2))
# plot(bss_lm)
# glance(bss_lm)
# glance(bss_lm)
# bss_lm_mse <- mean(residuals(bss_lm) ^ 2)
# bss_lm_mse
# bss_lm_rmse <- sqrt(bss_lm_mse)
# bss_lm_rmse
# bss_lm_rss <- sum(residuals(bss_lm)^2)
# bss_lm_rse <- summary(bss_lm)$sigma
# bss_lm_rse
# fit <- predict(bss_lm,bss_val)
# summary(fit)
# fit_ssl <- sum((bss_val$volume - fit)^2)
# sprintf("SSL/SSR/SSE: %f", fit_ssl)
# fit_mse <- fit_ssl/nrow(bss_val)
# sprintf("MSE: %f", fit_mse)
# fit_rmse <- sqrt(fit_mse)
# sprintf("RMSE: %f", fit_mse)
# bss_val$pred_vol <- fit
#
# pred_plot <- bss_val %>% ggplot(aes(volume,pred_vol)) + geom_point(alpha = 0.75) + geom_smooth(method = "loess") + stat_smooth(aes(color = "Predicted Volume")) + xlab("Actual Volume") + ylab("Predicted Volume")
# pred_plot
##############################################################################################
# library("zoo") # for interpolation
# library("mice") # for imputation purposes
# library("naniar") # for determining the nature of the missing data
# setwd("C:/Users/anubh/Desktop/Anubhav Shankar/Additional Education/INFORMS/BSS-Competition")
# getwd()
# bss <- read.csv("competition_training_data.csv") # Ingest the data set
# change_to_dbl <- function(df){
#   if(is.integer(df$column)){
#     return(as.double(df$column))
#   } else {
#     return (df$column)
#   }
# }
# bss_refresh <- bss
# x <- bss %>% filter(first_sell == "2022-01-01")
# length(unique(x$sku))
# length(unique(x$prod_cat))
# u <- as.list(unique(x$sku))
# View(u)
# Calculate the volume sold
# bss$vol <- 0.0
# bss$vol <- (bss$sales)/(bss$price)
# bss <- bss %>% mutate(vol = round(vol,1))
# comp_price_cols <- grep("comp_[0-9]+_price", colnames(bss_train), value = TRUE)
# nomiss_comp_prices <- rowSums(!is.na(bss_train[, comp_price_cols]))
# bss$prod_cat <- trimws(bss$prod_cat) # Remove the whitespace
# index <- sample(1:nrow(bss), 0.8*nrow(bss), replace = FALSE) # Create the data subset split
# blank_count <- bss_refresh %>% summarize_all(funs(sum(is.na(.)))) # Check the NA's in each field
# print(blank_count) # Just an added check, the counts here should match with the counts of
#                    # the 'is_na_count' object
# bss_rc__train_missing <- bss_train %>% summarise(across(where(is.numeric), ~sum(is.na(.x)))) %>% t() %>% as.data.frame()
# bss_rc_train_nomiss <- bss_train %>% summarise(across(where(is.numeric), ~sum(!is.na(.x)))) %>% t() %>% as.data.frame()
# Create a histogram to see the spread of the 'cost' field
# ggplot(bss, aes(x = cost)) +
#   # geom_histogram(aes(y = after_stat(density)),binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
#   geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
#   theme_minimal() +
#   labs(title = "Distribution of Cost", x = "Cost", y = "Count")
# Attempt to visualize the sales function
# plot(bss$price,bss$volume)
#
# ggplot(bss, aes(x = price, y = volume)) +
#   geom_line() +
#   labs(title = "Sales Function", x = "Price", y = "Volume") +
#   theme_minimal()
# n_distinct(bss_refresh$sku) # Check the number of distinct items
# sum(is.na(bss_refresh$comp_1_price))
# sum(is.na(bss_refresh$comp_2_price))
# sum(is.na(bss_refresh$comp_3_price))
# sum(is.na(bss_refresh$comp_4_price))
# sum(is.na(bss_refresh$comp_5_price))
# sum(is.na(bss_refresh$comp_data_max_price))
# sum(is.na(bss_refresh$comp_data_min_price))
# sum(is.na(bss_refresh$min_price))
# sum(is.na(bss_refresh$max_price))
# bss_temp$scaled_price_m <- round((bss_temp$price - bss_temp$min_price)/(bss_temp$max_price - bss_temp$min_price),2)
# bss_temp$scaled_price_c <- round((bss_temp$price - bss_temp$comp_data_min_price)/(bss_temp$comp_data_max_price - bss_temp$comp_data_min_price),2)
# # Scale all the price columns with the competitor & market thresholds
# bss <- bss %>%
#   mutate(
#     across(all_of(price_cols),
#            ~ ifelse(is.infinite(round((. - min_price) / (max_price - min_price),2)),
#                     NA,
#                     round((. - min_price) / (max_price - min_price),2)),
#            .names = "{.col}_scaled_m"),
#
#     across(all_of(price_cols),
#            ~ ifelse(is.infinite(round((. - comp_data_min_price) / (comp_data_max_price - comp_data_min_price),2)),
#                     NA,
#                     round((. - comp_data_min_price) / (comp_data_max_price - comp_data_min_price),2)),
#            .names = "{.col}_scaled_c")
#   )
# # Identify all the columns that contain price information, including 'price'
# price_columns <- c("price", grep("_price$", colnames(bss), value = TRUE))
# To ascertain the nature of the missing data
# group1 <- bss_refresh[!is.na(bss_refresh$comp_1_price), 'max_price']
# group2 <- bss_refresh[is.na(bss_refresh$comp_1_price), 'max_price']
# # Conduct a T-test
# t_test_result <- t.test(group1, group2)
# # Print the result
# print(t_test_result)
# # Plot histograms
# hist(group1, main="Group 1 (No Missing in comp_1_price)", xlab="max_price")
# hist(group2, main="Group 2 (Missing in comp_1_price)", xlab="max_price")
# # Create a new variable to indicate the group
# bss_refresh$group <- ifelse(is.na(bss_refresh$comp_1_price), "Missing", "No Missing")
# # Create a boxplot
# boxplot(max_price ~ group, data=bss_refresh, main="Comparison of max_price", ylab="max_price")
# Do a basic time-series EDA of the sales over the entire period
# bss_temp <- bss_refresh %>% select(salesdate,sales) # Create a temporary subset
# bss_ts <- ts(bss_temp$sales,frequency = 365) # Capture the yearly seasonality
# # Visualize the inherent seasonality
# plot(bss_ts)
# plot.ts(bss_ts)
# # Check stationarity
# adf.test(diff(bss_ts))
# acf(bss_ts)
# plot(diff(bss_ts)) # Plot the data after rendering it stationary
# # diff(bss_ts)
# fit <- stl(bss_ts,s.window = "period")
# plot(fit)
# additive_fit <- decompose(bss_ts,type = "additive")
# multiplicative_fit <- decompose(bss_ts,type = "multiplicative")
# plot(bss_ts,ylab = "Sales")
# # Summing up unitsordered for each weekday
# weekday_totals <- aggregate(bss_train$volume, by=list(weekday=bss_train$weekday), FUN=sum)
# # Sorting the data in descending order based on the total volume
# weekday_totals <- weekday_totals[order(weekday_totals$x), ]
# # Creating a barplot
# barplot(weekday_totals$x, names.arg = weekday_totals$weekday, main = "Total Units Ordered by Weekday",
# xlab = "Weekday", ylab = "Total Units Ordered", col = "steelblue")
# text(x=1:length(bss_train$weekday), y=bss_train$volume + 2, labels=bss_train$volume, cex=1, pos=3)
# #Quick Check
# bss_temp <- bss %>% select(sku,salesdate,revenue,cost,profit)
# bss_temp$diff <- bss_temp$revenue - bss_temp$cost # Get a new 'profit' column
# #View(bss_temp)
# bss_temp$check <- (bss_temp$profit != bss_temp$diff) # A logical field to check if the difference
# # in aggregated 'cost' field is different from the defined 'profit' field provided
# table(bss_temp$check) # If the pivot only has 'FALSE' entries our logic of combining the costs is not harmful/wrong
# rm(bss_temp) # remove the temporary data frame afterwards
# bss <- bss[,-c(6:9)] # Drop the separate costs
#
#
# # As the 'comp_x_price' columns have multiple NA's we'll impute median values to
# # replace the NA's. Median is better suited for it's lack of volatility and takes into
# # account the inherent skewness in the 'comp_X_price' fields.
#
# # Create a function to aid the imputation in multiple columns
# median_impute <- function(column) {
# if (is.numeric(column)) {
# return(ifelse(is.na(column), median(column, na.rm = TRUE), column))
# } else {
# return(column)  # return as-is for non-numeric columns
# }
# }
#
# # A vector of column names that need to be imputed
# cols_to_impute <- c("comp_1_price", "comp_2_price", "comp_3_price", "comp_4_price", "comp_5_price", "comp_data_min_price","comp_data_max_price","managed_fba_stock_level")
# # Use vapply() to execute the created function on the requisite columns
# bss_refresh[cols_to_impute] <- vapply(bss_refresh[cols_to_impute],median_impute, FUN.VALUE = numeric(nrow(bss_refresh)))
# # Check to see the successful implementation
# blank_count <- bss_refresh %>% summarize_all(funs(sum(is.na(.))))
# blank_count
# Imputation using interpolation (FOR EXPERIMENT ATM)
# na_count_train <- bss_train %>% summarize_all(funs(sum(is.na(.)))) # Check the NA's in each field
# na_count_train
#
# interpolate_impute <- function(col_name){
# if(is.numeric(col_name)){
# return(ifelse(is.na(col_name), na.approx(col_name), col_name))
# }
# else{
# return(col_name)
# }
# }
# summary(bss_pivot$price)
# summary(bss$price)
# bss_temp <- bss_temp %>% group_by(sku) %>% mutate(days_sold = round(as.numeric(difftime(max(salesdate), min(salesdate), units = "days"))))
#
# View(bss_temp)
# length(unique(bss_temp$days_sold))
# max(bss_temp$days_sold)
# min(bss_temp$days_sold)
# summary(bss_temp$days_sold)
# bss_temp <- bss_temp %>% group_by(sku) %>% mutate(first_sell = min(salesdate))
# bss_temp <- bss_temp %>% group_by(sku) %>% mutate(last_sell = max(salesdate))
# length(unique(bss_temp$first_sell))
# length(unique(bss_temp$last_sell))
# unique(bss_temp$first_sell)
#
# # Scale all the price columns with the competitor & market thresholds
# bss_temp <- bss_temp %>%
# mutate(
#   across(all_of(price_cols),
#        ~ ifelse(is.infinite(round((. - min_price) / (max_price - min_price),2)),
#                           NA,
#                           round((. - min_price) / (max_price - min_price),2)),
#           .names = "{.col}_scaled_m"),
#
#   across(all_of(price_cols),
#     ~ ifelse(is.infinite(round((. - comp_data_min_price) / (comp_data_max_price - comp_data_min_price),2)),
#                      NA,
#                      round((. - min_price) / (max_price - min_price),2)),
#         .names = "{.col}_scaled_c")
#   )
# summary(bss$volume)
# summary(bss$stock)
#
#
# y <- bss_val %>% filter(volume == 0)
# View(y)
# unique(y$pred_vol)
#
# bss_train[cols_to_impute] <- vapply(bss_train[cols_to_impute],interpolate_impute,FUN.VALUE = numeric(nrow(bss_train)))
#
# na_count_train <- bss_train %>% summarize_all(funs(sum(is.na(.)))) # Check the NA's in each field
# na_count_train
# View(bss_train)
# View(bss)
# Extract the minimum value from each price column
# min_prices <- bss %>%
#   select(all_of(price_cols)) %>%
#   summarise(across(everything(), min, na.rm = TRUE))
# # Convert to named vector
# min_prices_vector <- unlist(min_prices)
# min_prices_vector
# trend_temp <- ts(bss_temp$sales,start = c(2022,1),end = c(2023,3),frequency = 12)
# plot(trend_temp,xlab = "Year", ylab = "Sales", ylim = range(bss_temp$sales), bty = "l")
# Extract the maximum value from each price column
# max_prices <- bss %>%
#   select(all_of(price_cols)) %>%
#   summarise(across(everything(), max, na.rm = TRUE))
# # Convert to named vector
# max_prices_vector <- unlist(max_prices)
# max_prices_vector
# # Compute Q1 and Q3
# Q1 <- quantile(bss_train$price, 0.25, na.rm = TRUE)
# Q3 <- quantile(bss_train$price, 0.75, na.rm = TRUE)
# # Filter rows based on the IQR
# bss_train_f <- bss_train %>%
# filter(price >= Q1 & price <= Q3)
# maxp <- bss_train$max_price
# maxp <- max(bss_train$max_price)
# minp <- min(bss_train$min_price)
# d_minmax <- bss_train %>% filter(price>minp & price<maxp)
# bss_temp_f <- bss_temp %>% filter(first_sell != "2022-01-01")
# length(unique(bss_temp_f$sku))
# bss_temp_f <- bss_temp %>% select(sku,days_sold,first_sell,last_sell) %>% filter(first_sell != "2022-01-01")%>% group_by(sku)
# max_prices_f <- bss_f %>% select(prod_cat,all_of(price_cols),competitor_count) %>%
# group_by(prod_cat) %>% summarise(across(everything(), ~ifelse(all(is.na(.)),NA,max(., na.rm = TRUE))))
# min_prices_f <- bss_f %>% select(prod_cat,all_of(price_cols),competitor_count) %>%
# group_by(prod_cat) %>% summarise(across(everything(), ~ifelse(all(is.na(.)),NA,min(., na.rm = TRUE))))
# bss_temp <- bss
# glimpse(bss_temp)
# seed <- 456
# set.seed(seed)
# sample_rows <- sample(1:nrow(bss_temp),5)
# bss_sample <- bss_temp[sample_rows, ]
# glimpse(bss_sample)
# bss_cf <- bss %>% filter(prod_cat == "Classification Folders" & competitor_count >= 3)
# length(unique(bss_cf$sku))
# unique(bss_cf$sku)
# latest_date <- as.Date(max(bss_bkp$salesdate))
# new_dates <- seq(from = latest_date + 1, by = "day", length.out = 7)
# test <- data.frame()
# for(date in new_dates){
# daily_sample <- sample_n(bss_bkp,size = 10, replace = TRUE)
# # daily_sample$sku <- "File Folders SKU 47"
# daily_sample$salesdate <- as.Date(date)
# test <- rbind(test,daily_sample)
# }
#
# p <- predict(fit,test)
# test$pred_profit <- p
# bss_mixed <- lmer(volume ~ price_scaled_m + comp_1_price_scaled_m + comp_2_price_scaled_m + comp_3_price_scaled_m + comp_4_price_scaled_m + comp_5_price_scaled_m + (1 | prod_cat), data = bss_train, REML = TRUE )
# summary(bss_mixed)
#
# bss_mixed_1 <- lmer(volume ~ price_scaled_m + comp_1_price_scaled_m + comp_2_price_scaled_m + comp_3_price_scaled_m + comp_4_price_scaled_m + comp_5_price_scaled_m + (1 | sku), data = bss_train, REML = TRUE )
# summary(bss_mixed_1)
#
# bss_mixed_2 <- lmer(volume ~ salesdate + price_scaled_m + comp_1_price_scaled_m + comp_2_price_scaled_m + comp_3_price_scaled_m + comp_4_price_scaled_m + comp_5_price_scaled_m + (1 | sku), data = bss_train, REML = TRUE )
# summary(bss_mixed_2)
#
# bss_mixed_3 <- lmer(volume ~ price_scaled_m + comp_1_price_scaled_m + comp_2_price_scaled_m + comp_3_price_scaled_m + comp_4_price_scaled_m + comp_5_price_scaled_m + (1 + salesdate | sku), data = bss_train, REML = TRUE )
# summary(bss_mixed_3)
#
# bss_mixed_4 <- lmer(volume ~ price + (1 + salesdate | sku), data = bss_train, REML = TRUE )
# summary(bss_mixed_4)
# predict(bss_mixed_4, re.form = NA) %>% head()
# predict(bss_mixed_4) %>% head()
#
# predictInterval(bss_mixed_4)
# plotREsim(REsim(bss_mixed_4))
# plotREsim(REsim(bss_mixed_3))
#
# fit_me <- predict(bss_mixed_4,bss_val)
# bss_val$pred_vol <- fit_me
# temp_lm <- lm(price ~ pred_vol, data = temp)
# summary(temp_lm)
# temp$pred_profit <- (temp$pred_vol * temp$price) - temp$cost
# bss_hierachical_price_product <- lmer(volume ~ price + (1 + price | sku), data = bss_train, REML = TRUE )
# summary(bss_hierachical_price_product)
# ranef(bss_hierachical_price_product)
# bss_hierachical_volume <- lmer(price ~ 1 + volume + (1 + volume | sku) + (1 + volume | salesdate), data = bss_train, REML = TRUE)
# bss_hierachical_volume <- lmer(price ~ 1 + volume + (1 + volume | sku) + (1 + sku | salesdate), data = bss_train, REML = TRUE)
# bss_hierachical_volume <- lmer(price ~ 1 + volume + (1 + volume | sku), data = bss_train, REML = TRUE)
# summary(bss_hierachical_volume)
# ranef(bss_hierachical_volume)
# plot(ranef(bss_hierachical_volume))
#
# pred_price <- predict(bss_hierachical_volume, bss_val)
# bss_val$pred_price <- round(pred_price, 2)
# bss_val$pred_price <- sapply(bss_val$pred_price, round_to_constraint)
# bss_hierachical_date <- lmer(volume ~ price + (1 + salesdate | sku), data = bss_train, REML = TRUE )
# summary(bss_hierachical_date)
# val_lm <- lm(price ~ pred_vol, data = bss_val)
# summary(val_lm)
# temp <- bss_val %>% filter(sku == "File Folders SKU 47")
# #temp$est_profit <- round(((temp$pred_vol * temp$pred_price) - temp$cost), 2)
#
# fit <- lm(profit ~ pred_price, data = bss_val)
# summary(fit)
# fit_temp <- lm(profit ~ pred_price, data = temp)
# summary(fit_temp)
# x <- predict(fit_temp,temp)
# temp$est_profit <- x
# rmse <- sqrt(mean(residuals(fit_temp) ^ 2))
# rmse
#bss_hierachical_price_product_intercept <- lmer(volume ~ 1 + price + (1 + price | sku), data = bss_train, REML = TRUE)
# bss_hierachical_price_product_intercept <- lmer(volume ~ 1 + price + I(comp_1_price - price) + poly(salesdate,3) + (1 + poly(salesdate,3) + price | sku), data = bss_train, REML = TRUE)
# bss_hierachical_price_product_intercept <- lmer(volume ~ 1 + price + comp_1_price + comp_2_price + poly(salesdate,2) + weekday + (1 + poly(salesdate,2) + price | sku), data = bss_train, REML = TRUE)
# bss_hierachical_price_product_intercept <- lmer(volume ~ 1 + price + comp_1_price + comp_2_price + salesdate + (1 + price | sku), data = bss_train, REML = TRUE)
#bss_hierachical_price_product_intercept <- lmer(volume ~ 1 + price + comp_1_price + comp_2_price + salesdate + weekday + (1 + cost | sku), data = bss_train, REML = TRUE)
# profit_function <- function(price){
# price * predict(model, transform(df, price = price) - df$cost)
# }
# m <- mape(temp$price,temp$pred_price)
# p <- mape(temp$profit, temp$est_profit)
# print(paste0("Price MAPE: ", m, "%"))
# print(paste0("Profit MAPE: ", p, "%"))
# dev.off() # Clear residual plots, if any
