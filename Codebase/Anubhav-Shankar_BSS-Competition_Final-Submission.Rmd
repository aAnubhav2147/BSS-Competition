---
title: "Using a Hierarchical Model to Determine Optimal Price & Profit Forecasts for 2023 BSS Data Mining Challenge"
author: "Anubhav Shankar"
GitHub: "https://github.com/aAnubhav2147/BSS-Competition"
date: "`r Sys.Date()`"
output: html_document
---

**Problem Statement: **

_The primary objective of this competition is to predict the optimal daily market prices for eCommerce products sold by BSS, with the goal of maximizing the profit. At present, BSS employs manual methods to analyze market variables and competitors’ prices to make pricing decisions. The manual methods, while functional, are not highly efficient and time-consuming. Given the technological advancements and the vast amount of data available, we believe there is a significant opportunity to develop innovative data-driven methods to quickly analyze market trends and make informed pricing decisions._<br>

The dataset for this competition comprises a set of market and sales variables from office products sold on the BSS eCommerce platform. The provided dataset spans over two and a half years, providing a comprehensive look at the market dynamics of more than 200 office products.The profitability of each product is determined by several factors. These factors include eCommerce revenue and various internal
costs such as cost of goods sold (COGs), fulfillment fees, referral fees, and advertisement costs. BSS strategically adjusts product prices to maximize profitability, considering the prices set by competitors and customer demand responses.<br>

However, the dynamic market prices make it challenging for the company to
accurately predict the optimal price that will maximize profitability. The existing manual pricing decision process is not only time-consuming but also lacks the robustness that data-driven decision-making can potentially provide. <br>

Consequently, there is a significant need to develop a data-driven process. This process should ideally determine the market price that yields the maximum profitability by analyzing the eCommerce market data. In summary, this data challenge seeks innovative solutions to effectively address this problem with two main goals:<br>

• Determine the optimal selling price for an eCommerce product by leveraging more than two years of historical data comprising various sales and market variables.<br>
• Predict the expected daily profit for that product given a determined optimal selling price.<br>

The developed solution should ideally lead to more profitable pricing decisions and strategies in a dynamic
eCommerce market.<br>
<br>
_**Abstract**: In the rapidly evolving world of eCommerce, strategic pricing plays a crucial role in driving profitability and competitive advantage. Accurate pricing influences consumer behavior and impacts a company's bottom line. In industries with fierce competition and dynamic pricing, such as eCommerce, finding the optimal price for a product becomes a complex yet essential task. This task will be tackled by creating a hierarchical/mixed-effect model to predict the optimal volume and then optimize using a suitable profit function to obtain the optimal price. The optimal price will then be used to forecast the profit for the desired look ahead window._
<br>
```{r Load the requisite packages, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE) # A global setting to include outputs and suppress the warnings

library("readxl") # To read Excel files
library("tidyverse") # for tidy data 
library("car") # for regression
library("broom")
library("lubridate") # for working with date/time-stamp records
library("tseries") # for working with time-series data
library("forecast") # for working with time-series data
library("ggplot2") # for visualization
library("ggfortify") # For Visualization
library("lme4") # For building Hierarchical/Mixed-Effect Models
library("merTools")
library("stargazer") # For tabulating the results

```
<br>
```{r Data Ingestion, include=FALSE}

setwd("C:/Users/anubh/Desktop/Anubhav Shankar/Additional Education/INFORMS/BSS-Competition/Latest Data") # Absolute Path

bss <- read_excel("competition_data_2023_18_09.xlsx") # Ingest the data

# url <- "https://github.com/aAnubhav2147/BSS-Competition/blob/main/Latest%20Data/competition_data_2023_18_09.xlsx"
# 
# bss <- read.csv(url(url),header = TRUE) # Using a relative path

```
<br>
Let's have a quick glimpse of the metadata and see a few summary statistics below.<br>
```{r Check Metadata & Data Characteristics, include=TRUE, echo=TRUE}

# Visualize the metadata
str(bss) 

head(bss) # Get a pivot of the first few rows
summary(bss) # Get a basic data summary

max(bss$salesdate) # Get the ending date of the data
min(bss$salesdate) # Get the starting date of the data
length(unique(bss$sku)) # Get the number of unique Product IDs

```
<br>
Let's generate a pivot of NA values present in each column
<br>
```{r Check NAs, include=TRUE, echo=TRUE}

is_na_count <- bss %>% ungroup() %>% summarize_all(~sum(is.na(.))) %>% as.data.frame()  # Check the NA's in each field
# Use the ~ in the above formula as funs() has been deprecated
is_na_count 

```
<br>
All the competitor prices and the BSS stock ('unitsordered') have a significant amount of missing values.

```{r Necessary Functions, include=FALSE, echo=FALSE}

# Function to change all integer types to double

change_to_dbl <- function(df) {
  
    df %>% mutate_at(vars(which(sapply(df, is.integer))), as.double)
  
}

# Function to calculate the Mean Absolute Percentage Error

mape <- function(actual,forecast){
  
  valid_rows <- which(actual != 0 & !is.na(actual) & !is.na(forecast))
  actual <- actual[valid_rows]
  forecast <- forecast[valid_rows]
  
  n <- length(actual)
  
  if(n==0){
    
    return (NA)
    
  }
  
  val <- sum(abs((actual - forecast)/actual))/n * 100
  
  return(round(val,2))
  
}

# Function to round the price to $0.05 or $0.09

round_to_constraint <- function(x){
  integer_part <- floor(x) # Extract the integer part of the amount
  decimal_part <- x - integer_part # Extract the decimal part of the amount
  
  if(decimal_part < 0.05){
      return(integer_part + 0.05)
  }
  
  else if(decimal_part < 0.09){
    return(integer_part + 0.09)
  }
  
  # else if(decimal_part == 0.05 || decimal_part == 0.09){
  #   return(integer_part)
  # }
  
  # else {
  #   return(integer_part)
  # }
  
  else{
    return(integer_part + 0.05 + 1) # Best way to constraint for the decimal part >                                        0.09
    # Basically, we're rounding it off to the next dollar for a nickel increment
  }
}

```

```{r Preliminary Setup, include=FALSE}

bss <- change_to_dbl(bss) # Change all integer types to double

# Rename the requisite columns
bss <- bss %>% rename("volume" = "unitsordered")
bss <- bss %>% rename("revenue" = "sales")
bss <- bss %>% rename("stock" = "managed_fba_stock_level")

bss$prod_cat <- trimws(gsub("SKU \\d+", "",bss$sku)) # Extract the unique product categories

# Change the requisite columns to factors for the Hierarchical Model
bss <- bss %>% mutate(prod_cat = as.factor(prod_cat),
                                  sku = as.factor(sku))


# A vector of all competitor price columns
comp_price_cols_n <- grep("comp_[0-9]+_price", colnames(bss), value = TRUE)
comp_price_cols <- grep("_price",colnames(bss),value = TRUE)
price_cols <- c("price",comp_price_cols_n) # Create a vector of BSS & Competitor price columns

bss$competitor_count <- rowSums(!is.na(bss[, comp_price_cols_n])) # Get the number of competitors for each product

# rowSums() in the above command is counting the number of columns that doesn't have a NA value for a particular row


# Scale all the price columns with the competitor & market thresholds
# bss <- bss %>%
#   mutate(
#     across(all_of(price_cols),
#            ~ ifelse(is.infinite(round((. - min_price) / (max_price - min_price),2)),
#                     NA,
#                     round((. - min_price) / (max_price - min_price),2)),
#            .names = "{.col}_scaled_m"),
#     
#     across(all_of(price_cols),
#            ~ ifelse(is.infinite(round((. - comp_data_min_price) / (comp_data_max_price - comp_data_min_price),2)),
#                     NA,
#                     round((. - comp_data_min_price) / (comp_data_max_price - comp_data_min_price),2)),
#            .names = "{.col}_scaled_c")
#   )

# The scaled prices could be a better predictor of how each product reacts to price &
# volume fluctuations.

# Merge all the costs together
bss$cost <- 0.0
bss$cost <- bss$cogs + bss$fba + bss$reffee + bss$adspend

# Do the necessary date operations
bss$salesdate <- as.Date(bss$salesdate,"%Y-%m-%d") # Coerce the 'salesdate' into Date type

bss$quarter <- as.factor(paste0("Q",quarter(bss$salesdate))) # Extract the Quarter from the date

bss$month <- factor(month(bss$salesdate, label = TRUE, abbr = TRUE), ordered = FALSE)# Extract the month from the date

bss$weekday <- factor(wday(bss$salesdate, label = TRUE), ordered = FALSE) # Introduce a weekday instrumental variable

bss <- bss %>% group_by(sku) %>% mutate(days_sold = round(as.numeric(difftime(max(salesdate), min(salesdate), units = "days")))) # Calculate the duration for which a good has been sold

bss <- bss %>% group_by(sku) %>% mutate(first_sell = min(salesdate)) # First instance of the good sold

bss <- bss %>% group_by(sku) %>% mutate(last_sell = max(salesdate)) # Latest instance of the good sold

# length(unique(bss$first_sell)) # See the different starting dates
# length(unique(bss$last_sell)) # See the different latest sell dates for products 

```

**Methodology**: A cursory glance through the abstract and the motivation sections might preordain using either a linear regression or a numerical method like extrapolation for the first objective and, consequently, use something like an Autoregressive Integrated Moving Average (ARIMA) for the second objective.

However, the methods above may fail to capture each product's inherent volatility and dynamic demand curves. So, a hierarchical/mixed effect model is more suitable for such a purpose. Additionally, a parametric model will be less computationally intensive and more intuitive.

**Abbreviations and Acronyms**:

The following abbreviations will be used extensively throughout the document for brevity:<br> 
i.)	ME -> Mixed Effect<br>
ii.)	ARIMA -> Autoregressive Integrated Moving Average<br>
iii.)	LME -> Linear Mixed-Effect<br>
iv.)	OLS -> Ordinary Least-Squares<br>
v.)	RE -> Random Effect (can be used interchangeably with (i) & (iii))<br>
vi.)	FFSKU47 -> File-Folders SKU 47<br>

Before we proceed further let's have a look at the time-series nature of the dataset and some preliminary insights.
<br>
```{r Check the data seasonality, include=TRUE}

bss_season <- bss %>% dplyr::select(salesdate,revenue) %>% group_by(sku) # Create a temporary subset
bss_ts <- ts(bss_season$revenue,frequency = 365) # Capture the yearly seasonality

fit <- stl(bss_ts,s.window = "period")
plot(fit)
additive_fit <- decompose(bss_ts,type = "additive")
multiplicative_fit <- decompose(bss_ts,type = "multiplicative")
plot(bss_ts,ylab = "Sales")

```
<br>
_After checking the 'trend' plot, we can confidently say that the seasonality of the plot is **additive**_.
<br>
```{r Basic Time-Series EDA, include=TRUE}

# Do a basic time-series EDA of the sales over the entire period

# Visualize the inherent seasonality 
# plot(bss_ts)
plot.ts(bss_ts)

# Check stationarity
adf.test(diff(bss_ts)) # Augmented-Dickey Fuller Test
acf(bss_ts) # Auto-Correlation Factor plot

plot(diff(bss_ts)) # Plot the data after rendering it stationary

```
<br>
After looking at the results of the **Augmented Dickey-Fuller Test** and the **Auto-Correlation Factor** plot, we can confidently say that the data is **stationary**.
```{r Preliminary EDA, include=TRUE}

# Verify to see if the competitor counts match the problem document
comp_count_verification <- bss %>%
  group_by(sku) %>%
  summarise(unique_competitor_count = unique(competitor_count)) %>%
  group_by(unique_competitor_count) %>%
  summarise(num_of_items = n()) %>%
  arrange(unique_competitor_count)
comp_count_verification


# Visualize the competition
bss_temp <- bss %>% dplyr::select(prod_cat, price, all_of(comp_price_cols),competitor_count) %>% group_by(prod_cat)
total_competitors_by_product <- aggregate(bss_temp$competitor_count, by=list(product_name=bss_temp$prod_cat), FUN=max)
colnames(total_competitors_by_product)[2] <- "competitors" # Rename the second element of the list; 'x' by default
ggplot(total_competitors_by_product,aes(x=product_name, y=competitors)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  # geom_smooth(aes(group=1), method="loess", color="red", se=FALSE) +
  geom_text(aes(label = competitors), vjust = -0.5) +
  labs(title = "Max No. of Competitors by Product",
       x = "Product",
       y = "No. of Competitors") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1.2, size = 8)) # Rotate the axis labels to make them more legible


# Check the distribution of the 'volume' field
weekday_totals <- aggregate(bss$volume, by=list(weekday=bss$weekday), FUN=sum)
colnames(weekday_totals)[2] <- "volume"
ggplot(weekday_totals,aes(x=weekday, y=volume)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_smooth(aes(group=1), method="loess", color="red", se=FALSE) +
  geom_text(aes(label = volume), vjust = -0.5) +
  labs(title = "Volume sold by Weekday",
       x = "Weekday",
       y = "Volume") +
  theme_minimal()


# Check the distribution of the volume sold by the Product
product_totals <- aggregate(bss$volume, by=list(product_name=bss$prod_cat), FUN=sum)
colnames(product_totals)[2] <- "volume" # Rename the second element of the list; 'x' by default
ggplot(product_totals,aes(x=product_name, y=volume)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  # geom_smooth(aes(group=1), method="loess", color="red", se=FALSE) +
  geom_text(aes(label = volume), vjust = -0.5) +
  labs(title = "Volume sold by Product",
       x = "Product",
       y = "Volume") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1.2, size = 8)) # Rotate the axis labels to make them more legible


# Filter for data in 2022
recent_profit <- bss %>%
  filter(year(salesdate) == 2022) %>%
  # Group by quarter and summarize the profit
  group_by(Qtr = quarter) %>%
  summarise(Total_Profit = sum(profit, na.rm = TRUE)) %>%
  ungroup()
# Convert the data to a time series object
profit_ts <- ts(recent_profit$Total_Profit, start = c(2022,1), frequency = 4)
# Plot
autoplot(profit_ts) + labs(y ="Profit Earned", title = "Quarterly Profits for 2022")


recent_sales <- bss %>%
  filter(year(salesdate) == 2022) %>%
  # Group by quarter and summarize the sales
  group_by(Qtr = quarter) %>%
  summarise(Total_Sales = sum(volume, na.rm = TRUE)) %>%
  ungroup()
sales_ts <- ts(recent_sales$Total_Sales, start = c(2022,1), frequency = 4)
autoplot(sales_ts) + labs(y ="Volume Sold", title = "Quarterly Sales for 2022")

monthly_sales <- bss %>%
  filter(year(salesdate) == 2022) %>%
  # Group by quarter and summarize the sales
  group_by(Month = month) %>%
  summarise(Total_Sales_m = sum(volume, na.rm = TRUE)) %>%
  ungroup()
monthly_sales_ts <- ts(monthly_sales$Total_Sales_m, start = c(2022,1), frequency = 12)
autoplot(monthly_sales_ts) + labs(y ="Volume Sold", title = "Monthly Sales for 2022")


```
<br>
The figures above show that sales and profits fell sharply around the end of August. Additionally, the volume distribution on the day-to-day sales shows the sales distribution is nearly normal, with Saturday as the worst-performing day. Such nuances will be useful to control for.<br>

Additionally, there are nearly **230 individual products** that are sold. However, they can be **grouped into nine (9) distinct types/categories**, as shown in the graph above that tracks the sales performance of each category. **‘File Folders’** is the **best-performing product type/category**. Concurrently, we also know that each product can have multiple competitors. A bird's eye view of the competition indicates that in addition to being the best-performing product category, **‘File Folders’ also has the highest number of competitors**.<br> 

As observed earlier, all the competitor prices have a large amount of missing values. Let's see the distribution of the seller price and the competitor prices respectively.
<br>
```{r Price Dynamics, include= TRUE}

# Create a histogram to see the spread of the 'price' field
ggplot(bss, aes(x = price)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  geom_density(color = "red", linewidth = 1) + # plot a superimposing density curve 
  theme_minimal() +
  labs(title = "Distribution of Price", x = "Price", y = "Density")

# Let's see the distribution of all the price variables 

# Convert data to long format
bss_long <- bss %>%
  pivot_longer(cols = contains("_price"), # A regex operation
               names_to = "Price_Variable",
               values_to = "Price_Value",
               values_drop_na = TRUE)

# Plot using ggplot2
ggplot(bss_long, aes(x = Price_Value)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7, bins = 10) +
  facet_wrap(~Price_Variable, scales = "free_y") + # Customize each plot basis it's respective range
  theme_minimal() +
  labs(title = "Distribution of Price Variables", x = "Price", y = "Count")

```
<br>
All prices seem to have a noticeable and distinct right skew. Concurrently, there have been instances where either BSS or one of the competitor seems to have sold a product for **$0 or charged a price near the market upper threshold**.<br>

While these overarching insights are helpful, let us dive deeper and see the demand curves for the ‘File Folders’ category and our assigned ‘sku’, ‘FFSKU 47' for a snapshot in time.<br>
```{r Product Dynamics, include= TRUE}

bss_pivot <- bss %>% filter(salesdate >= "2022-01-01" & salesdate <= "2022-08-31") %>% dplyr::select(sku, price, revenue, volume, prod_cat) %>% distinct()


product_category <- c("File Folders", "Classification Folders")
bss_pivot %>% filter(prod_cat %in% product_category) %>%
  ggplot(aes(x = price, y = volume, color = prod_cat)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(title = "Demand Curve for Comparable Product Categories",
       x = "Price",
       y = "Volume",
       color = "Product") +
  theme_minimal()

product_category <- c("File Folders")
bss_pivot %>% filter(prod_cat %in% product_category) %>%
  ggplot(aes(x = price, y = volume, color = prod_cat)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(title = "Demand Curve for File-Folders",
       x = "Price",
       y = "Volume",
       color = "Product") +
  theme_minimal()

# product_name <- c("File Folders SKU 47","File Folders SKU 20")
product_name <- c("File Folders SKU 47")
bss_pivot %>% filter(sku %in% product_name) %>%
  ggplot(aes(x = price, y = volume, color = sku)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(title = "Demand Curve for FFSKU47",
       x = "Price",
       y = "Volume",
       color = "Product") +
  theme_minimal()

# bss_pivot <- bss %>% filter(salesdate >= "2022-01-01" & salesdate <= "2022-07-13") %>% dplyr::select(sku, price, revenue, prod_cat) %>% distinct()
product_name <- c("File Folders SKU 47")
bss_pivot %>% filter(sku %in% product_name) %>%
ggplot(aes(x = price, y = revenue, color = sku)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
labs(title = "Price vs Revenue for Selected Product",
x = "Price",
y = "Revenue",
color = "Product") +
theme_minimal()

```
<br>
_Right away, we observe a nuance. While the demand curve for ‘File Folders’ as a category shows a nominal spread with a  noticeable upward slope, the curve for FFSKU47 shows a higher spread and has a practically flat slope_.
<br>
_Additionally, we see another interesting phenomenon. For the same spread, we observe an upward-sloping relationship between price and revenue, indicating that the **revenue clocks an increase for a unit increase in the price!**_ <br>

We'll create the training and validation sets below.<br>

```{r Split into training and validation set, include=TRUE}

seed <- 123
set.seed(seed = seed) # Set seed for reproducibility 

bss_train <- bss %>% filter(salesdate <= "2023-05-31") # Create the training set
bss_val <- bss %>% filter(salesdate >= "2023-06-01" & sku == "File Folders SKU 47") # Create the validation set

glimpse(bss_train)

glimpse(bss_val)


```

**Introduction**: The ‘unitsordered’ (renamed as ‘volume’ for the analysis) has a high level of variability, which the seller can control for. We can maximize the daily profit by accurately predicting the volume sold and optimizing for the price with a relevant profit function. To accurately predict the volume, we’ll make a hierarchical model controlling for the cost and individual ‘sku’ to capture the dynamic demand curves. The provided data will be split into training and validation sets, respectively, to create the model, and the model subsequently will be tested on a randomly generated test set having the same distributional assumptions as the original data to forecast the daily profit prediction for the ‘t+6’ lookahead window.

**Mixed Effect Models & Approach**: Also known as Linear Mixed-Effect models, it is primarily used for modeling a mixture of fixed and random effects. Fixed effects refer to the typical main effects one would see in a linear regression model, i.e., the non-random part of a mixed model. In some contexts, they are referred to as the population average effect. Random effects are simply those specific to an observational unit, however defined. In our case, the observational units would be the products themselves.

The lmer() command, part of the ‘lme4’ package, can run an ME model in R [1]. 

We will create an ME model to predict the optimal volume and optimize it using the optim() command in the lme4 package to get an optimal price point.

Mathematically, 

         Volume = 1 + β1 * X1 + β2 * X2  + β3 * X3 + β4 * X4  + β5 * X5  +  β6 * X6  +  γ  * X7  +  εi                               - (1)

       In Eq(1), X1, X2, …… X7 -> The independent variables on which the dependent variable, ‘volume,’ will be modeled.

where,<br>
	       εi -> The error/residual term<br>
  		       X1 -> comp_1_price*<br>
	       X2 -> comp_2_price*<br>
	       X3 -> cost<br>
	       X4 -> Markup; coded as I(price-cost)<br>
	       X5 -> salesdate<br>
	       X6 -> weekday<br>
	       X7 -> RE of cost for each ‘sku’; coded as (1 + cost | sku), the ‘1’ represents the intercept for the RE <br>

**Note: While the File Folders product category has five competitors, FFSKU47 only has two. Hence, only two competitor prices have been included in the model**.
<br>
```{r Hierarchical Model, include=TRUE}

bss_hierachical_cost_product_intercept <- lmer(volume ~ 1 + comp_1_price + comp_2_price + cost + I(price - cost) + salesdate + weekday + (1 + cost | sku), data = bss_train, REML = TRUE) # Hierarchical/Mixed-Effect Model

ranef(bss_hierachical_cost_product_intercept) # Coefficients of the random effects
plot(ranef(bss_hierachical_cost_product_intercept)) # Plots the cost vs. intercept variance
summary(bss_hierachical_cost_product_intercept) # Coefficients of the mixed & random effects

predictions <- predict(bss_hierachical_cost_product_intercept, newdata = bss_val)
bss_val$pred_vol <- round(predictions,0)

stargazer(bss_hierachical_cost_product_intercept, type = "text", title = "Random Effect Results", summary = TRUE) # Tabulate the summary of the model

```
<br>
The profit function conducts a row-wise operation by multiplying the predicted volume with the optimal price and subtracting the static costs. The ME model and the profit function are then fed into the optim() command within the defined bounds to arrive at the optimal price point.<br>
```{r Profit Function & Optimization, include=TRUE}

# Create a profit function to get the maximum profit using the predicted volume and optimal price

profit_function <- function(price, model, df) {
  
            predicted_volume <- predict(model, newdata = transform(df, price = price)) # Predict the volume of sales for the given price by replacing the existing prices with the optimum prices using the transform() function
            
            profit <- price * predicted_volume - df$cost # A row-wise operation to calculate the max profit for each day
            
            return(-sum(profit, na.rm = TRUE)) #*
          #* Use the negative sign to maximize for the profit as optim() by default
          #  minimizes the function value.
            
}

# Optimizer Function

initial_guess <- median(bss_val$price, na.rm = TRUE) # Initial value for optimizing

# Set the bounds for the optimizer function
lower_limit <- min(bss_val$price, na.rm = TRUE)
upper_limit <- max(bss_val$price, na.rm = TRUE)

result <- optim(
  
    par = initial_guess,
    fn = profit_function,
    model = bss_hierachical_cost_product_intercept,
    df = bss_val,
    method = "L-BFGS-B", # A memory efficient high-dimensional optimization method that allows the set up of requisite constraint boundaries.
    lower = lower_limit,
    upper = upper_limit
    
)

t <- round_to_constraint(result$par) # Obtain the optimal price
print(paste0("Optimal Price: ", "$",t))

```
<br>

We'll now create a **randomly generated test set** having the same distributional assumptions as the original data to forecast the daily profit prediction for the **‘t+6’ lookahead window**. <br>
```{r Simulate Model Performance, include=TRUE}

temp <- bss_val %>% filter(salesdate > "2023-08-31" & salesdate <= "2023-09-11")

# Remove the last two rows 
if (nrow(temp) > 2) {
temp <- temp[-((nrow(temp) - 1):(nrow(temp))), ] #Deletes the last two rows
} else {
# Handle the case where you have two or fewer rows
warning("Data frame has two or fewer rows. Cannot remove last two rows.")
}

temp$salesdate <- seq(from = as.Date("2023-09-17"), to = as.Date("2023-09-25"), by = "day")
temp$weekday <- wday(temp$salesdate, label = TRUE) # Introduce a weekday instrumental variable
temp$price <- t
temp$pred_vol <- round(predict(bss_hierachical_cost_product_intercept, newdata = temp, re.form = NA),0)
temp$pred_profit <- (temp$price * temp$pred_vol) - temp$cost

vv <- mape(bss_val$volume, bss_val$pred_vol)
print(paste0("Volume MAPE_Validation: ", vv, "%"))

vt <- mape(temp$volume, temp$pred_vol)
print(paste0("Volume MAPE_Test: ", vt, "%"))

p <- mape(temp$profit, temp$pred_profit)
print(paste0("Profit MAPE: ", p, "%"))

```
<br>
**Result**: *From the ME model summary, we can see that the fixed-effect coefficient for the markup is negative and is thus indicative of the fact that for our particular ‘sku,’ a higher price may, in fact, be more profitable. Also, given that ‘comp_2_price’ is statistically significant, it augurs well that the optimal price point predicted is near that price without exceeding it while also being within the ‘max_price’ & ‘min_price*.’ **Also, the calculated MAPE on the simulated test set was 4.52%, while it was 21.1% for the validation set**.<br>
<br>
**Advantages of the Hierarchical Model**:

a.)	Less computationally intensive<br>
b.)	Intuitive and relatively easy to replicate<br>
c.)	The chosen variables lead to a relatively parsimonious model <br>
<br>
**Acknowledgement**: This project has been a challenging endeavor. I want to thank Neal Fultz, Statistical Scientist, UCLA (nfultz@ucla.edu), for his valuable insights and fine-tuning my approach to the problem by familiarizing me with hierarchical models.<br>
<br>
**References**:
 
1.)	Mixed Models with R <br>
2.)	Intro to R Stats: Hierarchical Linear Models<br>
3.)	Forecasting Principles & Practice (a) & (b)<br>
4.)	Hierarchical Linear Modeling (HLM): An Introduction to Cross-Sectional & Growth  Modeling Frameworks<br>
5.)	lme4: Linear Mixed-Effects Models<br>

