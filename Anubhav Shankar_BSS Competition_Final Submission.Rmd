---
title: "BSS Competition"
author: "Anubhav Shankar"
date: "`r Sys.Date()`"
output: html_document
---

The primary objective of this competition is to predict the optimal daily market prices for eCommerce products sold by BSS, with the goal of maximizing the profit. At present, BSS employs manual methods to analyze market variables and competitors’ prices to make pricing decisions. The manual methods, while functional, are not highly efficient and time-consuming. Given the technological advancements and the vast amount of data available, we believe there is a significant opportunity to develop innovative data-driven methods to quickly analyze market trends and make informed pricing decisions.<br>

The dataset for this competition comprises a set of market and sales variables from office products sold on the BSS eCommerce platform. The provided dataset spans over two and a half years, providing a comprehensive look at the market dynamics of more than 200 office products.The profitability of each product is determined by several factors. These factors include eCommerce revenue and various internal
costs such as cost of goods sold (COGs), fulfillment fees, referral fees, and advertisement costs. BSS strategically adjusts product prices to maximize profitability, considering the prices set by competitors and customer demand responses. <br>

However, the dynamic market prices make it challenging for the company to
accurately predict the optimal price that will maximize profitability. The existing manual pricing decision process is not only time-consuming but also lacks the robustness that data-driven decision-making can potentially provide. <br>

Consequently, there is a significant need to develop a data-driven process. This process should ideally determine the market price that yields the maximum profitability by analyzing the eCommerce market data. In summary, this data challenge seeks innovative solutions to effectively address this problem with two main goals:
<br>
• Determine the optimal selling price for an eCommerce product by leveraging more than two years of historical data comprising various sales and market variables.<br>
• Predict the expected daily profit for that product given a determined optimal selling price.<br>
The developed solution should ideally lead to more profitable pricing decisions and strategies in a dynamic
eCommerce market.
<br>
```{r Load the requisite packages, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE) # A global setting to include outputs and suppress the warnings

library("tidyverse") # for tidy data 
library("car") # for regression
library("broom")
library("lubridate") # for working with date/time-stamp records
library("tseries") # for working with time-series data
library("forecast") # for working with time-series data
library("ggplot2") # for visualization
library("ggfortify") # For Visualization
library("lme4") # For building Hierarchical/Mixed-Effect Models
library("merTools")

```
<br>
```{r Set Working Directory, include=FALSE}
setwd("C:/Users/anubh/Desktop/Anubhav Shankar/Additional Education/INFORMS/BSS-Competition") # Absolute Path
getwd()
```
<br>
```{r Data Ingestion, include=FALSE, echo=TRUE}

url <- "https://raw.githubusercontent.com/aAnubhav2147/BSS-Competition/main/Archive/Archived%20Datasets/competition_training_data_09112023.csv"

bss <- read.csv(url(url),header = TRUE) # Ingest the data set

# Visualize the metadata
str(bss) 
glimpse(bss)

head(bss) # Get a pivot of the first few rows
summary(bss) # Get a basic data summary

max(bss$salesdate) # Get the ending date of the data
min(bss$salesdate) # Get the starting date of the data
length(unique(bss$sku)) # Get the number of unique products

```
<br>
Let's generate a pivot of NA values present in each column
<br>
```{r Check NAs, include=TRUE, echo=TRUE}
is_na_count <- bss %>% summarize_all(funs(sum(is.na(.)))) # Check the NA's in each field
print(is_na_count)
```
<br>
All the competitor prices and the BSS prices have a significant amount of missing values.
<br>
```{r Necessary Functions, include=FALSE, echo=FALSE}

# Function to change all integer types to double

change_to_dbl <- function(df) {
  
    df %>% mutate_at(vars(which(sapply(df, is.integer))), as.double)
  
}

# Function to calculate the Mean Absolute Percentage Error

mape <- function(actual,forecast){
  
  valid_rows <- which(actual != 0 & !is.na(actual) & !is.na(forecast))
  actual <- actual[valid_rows]
  forecast <- forecast[valid_rows]
  
  n <- length(actual)
  
  if(n==0){
    
    return (NA)
    
  }
  
  val <- sum(abs((actual - forecast)/actual))/n * 100
  
  return(round(val,2))
  
}

# Function to round the price to $0.05 or $0.09

round_to_constraint <- function(x){
  integer_part <- floor(x) # Extract the integer part of the amount
  decimal_part <- x - integer_part # Extract the decimal part of the amount
  
  if(decimal_part < 0.05){
      return(integer_part + 0.05)
  }
  
  else if(decimal_part < 0.09){
    return(integer_part + 0.09)
  }
  
  # else if(decimal_part == 0.05 || decimal_part == 0.09){
  #   return(integer_part)
  # }
  
  # else {
  #   return(integer_part)
  # }
  
  else{
    return(integer_part + 0.05 + 1) # Best way to constraint for the decimal part >                                        0.09
    # Basically, we're rounding it off to the next dollar for a nickel increment
  }
}

```

<br>

<br>

<br>
```{r Preliminary Setup, include=FALSE}

bss <- change_to_dbl(bss) # Change all integer types to double

# Rename the requisite columns
bss <- bss %>% rename("volume" = "unitsordered")
bss <- bss %>% rename("revenue" = "sales")
bss <- bss %>% rename("stock" = "managed_fba_stock_level")

bss$prod_cat <- trimws(gsub("SKU \\d+", "",bss$sku)) # Extract the unique product categories

# Change the requisite columns to factors for the Hierarchical Model
bss <- bss %>% mutate(prod_cat = as.factor(prod_cat),
                                  sku = as.factor(sku))


# A vector of all competitor price columns
comp_price_cols_n <- grep("comp_[0-9]+_price", colnames(bss), value = TRUE)
comp_price_cols <- grep("_price",colnames(bss),value = TRUE)
price_cols <- c("price",comp_price_cols_n) # Create a vector of BSS & Competitor price columns

bss$competitor_count <- rowSums(!is.na(bss[, comp_price_cols_n])) # Get the number of competitors for each product

# rowSums() in the above command is counting the number of columns that doesn't have a NA value for a particular row


# Scale all the price columns with the competitor & market thresholds
# bss <- bss %>%
#   mutate(
#     across(all_of(price_cols),
#            ~ ifelse(is.infinite(round((. - min_price) / (max_price - min_price),2)),
#                     NA,
#                     round((. - min_price) / (max_price - min_price),2)),
#            .names = "{.col}_scaled_m"),
#     
#     across(all_of(price_cols),
#            ~ ifelse(is.infinite(round((. - comp_data_min_price) / (comp_data_max_price - comp_data_min_price),2)),
#                     NA,
#                     round((. - comp_data_min_price) / (comp_data_max_price - comp_data_min_price),2)),
#            .names = "{.col}_scaled_c")
#   )

# The scaled prices could be a better predictor of how each product reacts to price &
# volume fluctuations.

# Merge all the costs together
bss$cost <- 0.0
bss$cost <- bss$cogs + bss$fba + bss$reffee + bss$adspend

# Do the necessary date operations
bss$salesdate <- as.Date(bss$salesdate,"%Y-%m-%d") # Coerce the 'salesdate' into Date type

bss$quarter <- as.factor(paste0("Q",quarter(bss$salesdate))) # Extract the Quarter from the date

bss$month <- factor(month(bss$salesdate, label = TRUE, abbr = TRUE), ordered = FALSE)# Extract the month from the date

bss$weekday <- factor(wday(bss$salesdate, label = TRUE), ordered = FALSE) # Introduce a weekday instrumental variable

bss <- bss %>% group_by(sku) %>% mutate(days_sold = round(as.numeric(difftime(max(salesdate), min(salesdate), units = "days")))) # Calculate the duration for which a good has been sold

bss <- bss %>% group_by(sku) %>% mutate(first_sell = min(salesdate)) # First instance of the good sold

bss <- bss %>% group_by(sku) %>% mutate(last_sell = max(salesdate)) # Latest instance of the good sold

# length(unique(bss$first_sell)) # See the different starting dates
# length(unique(bss$last_sell)) # See the different latest sell dates for products 

```
<br>
```{r Preliminary EDA, include=TRUE}

# Verify to see if the competitor counts match the problem document
comp_count_verification <- bss %>%
  group_by(sku) %>%
  summarise(unique_competitor_count = unique(competitor_count)) %>%
  group_by(unique_competitor_count) %>%
  summarise(num_of_items = n()) %>%
  arrange(unique_competitor_count)
comp_count_verification


# Visualize the competition
bss_temp <- bss %>% dplyr::select(prod_cat, price, all_of(comp_price_cols),competitor_count) %>% group_by(prod_cat)
total_competitors_by_product <- aggregate(bss_temp$competitor_count, by=list(product_name=bss_temp$prod_cat), FUN=max)
colnames(total_competitors_by_product)[2] <- "competitors" # Rename the second element of the list; 'x' by default
ggplot(total_competitors_by_product,aes(x=product_name, y=competitors)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  # geom_smooth(aes(group=1), method="loess", color="red", se=FALSE) +
  geom_text(aes(label = competitors), vjust = -0.5) +
  labs(title = "Max No. of Competitors by Product",
       x = "Product",
       y = "No. of Competitors") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1.2, size = 8)) # Rotate the axis labels to make them more legible


# Create a histogram to see the spread of the 'price' field
ggplot(bss, aes(x = price)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
  geom_density(color = "red", linewidth = 1) + # plot a superimposing density curve 
  theme_minimal() +
  labs(title = "Distribution of Price", x = "Price", y = "Density")

# Create a histogram to see the spread of the 'cost' field
ggplot(bss, aes(x = cost)) + 
  # geom_histogram(aes(y = after_stat(density)),binwidth = 10, fill = "blue", color = "black", alpha = 0.7) + 
  geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) + 
  theme_minimal() + 
  labs(title = "Distribution of Cost", x = "Cost", y = "Count")


# Let's see the distribution of all the price variables 

# Convert data to long format
bss_long <- bss %>%
  pivot_longer(cols = contains("_price"), # A regex operation
               names_to = "Price_Variable",
               values_to = "Price_Value",
               values_drop_na = TRUE)

# Plot using ggplot2
ggplot(bss_long, aes(x = Price_Value)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7, bins = 10) +
  facet_wrap(~Price_Variable, scales = "free_y") + # Customize each plot basis it's respective range
  theme_minimal() +
  labs(title = "Distribution of Price Variables", x = "Price", y = "Count")


# Check the distribution of the 'volume' field
weekday_totals <- aggregate(bss$volume, by=list(weekday=bss$weekday), FUN=sum)
colnames(weekday_totals)[2] <- "volume"
ggplot(weekday_totals,aes(x=weekday, y=volume)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_smooth(aes(group=1), method="loess", color="red", se=FALSE) +
  geom_text(aes(label = volume), vjust = -0.5) +
  labs(title = "Volume sold by Weekday",
       x = "Weekday",
       y = "Volume") +
  theme_minimal()


# Check the distribution of the volume sold by the Product
product_totals <- aggregate(bss$volume, by=list(product_name=bss$prod_cat), FUN=sum)
colnames(product_totals)[2] <- "volume" # Rename the second element of the list; 'x' by default
ggplot(product_totals,aes(x=product_name, y=volume)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  # geom_smooth(aes(group=1), method="loess", color="red", se=FALSE) +
  geom_text(aes(label = volume), vjust = -0.5) +
  labs(title = "Volume sold by Product",
       x = "Product",
       y = "Volume") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1.2, size = 8)) # Rotate the axis labels to make them more legible

bss_pivot <- bss %>% filter(salesdate >= "2022-01-01" & salesdate <= "2022-07-13") %>% dplyr::select(sku, price, volume, prod_cat) %>% distinct()

# product_name <- c("File Folders SKU 47","File Folders SKU 20")
product_name <- c("File Folders SKU 47")
bss_pivot %>% filter(sku %in% product_name) %>%
  ggplot(aes(x = price, y = volume, color = sku)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(title = "Price vs Volume for Selected Product",
       x = "Price",
       y = "Volume",
       color = "Product") +
  theme_minimal()

product_category <- c("File Folders", "Classification Folders")
bss_pivot %>% filter(prod_cat %in% product_category) %>%
  ggplot(aes(x = price, y = volume, color = prod_cat)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(title = "Price vs Volume for File Folders",
       x = "Price",
       y = "Volume",
       color = "Product") +
  theme_minimal()

bss_pivot <- bss %>% filter(salesdate >= "2022-01-01" & salesdate <= "2022-07-13") %>% dplyr::select(sku, price, revenue, prod_cat) %>% distinct()
product_name <- c("File Folders SKU 47")
bss_pivot %>% filter(sku %in% product_name) %>%
ggplot(aes(x = price, y = revenue, color = sku)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
labs(title = "Price vs Revenue for Selected Product",
x = "Price",
y = "Revenue",
color = "Product") +
theme_minimal()


# Filter for data in 2022
recent_profit <- bss %>%
  filter(year(salesdate) == 2022) %>%
  # Group by quarter and summarize the profit
  group_by(Qtr = quarter) %>%
  summarise(Total_Profit = sum(profit, na.rm = TRUE)) %>%
  ungroup()
# Convert the data to a time series object
profit_ts <- ts(recent_profit$Total_Profit, start = c(2022,1), frequency = 4)
# Plot
autoplot(profit_ts) + labs(y ="Profit Earned", title = "Quarterly Profits for 2022")


recent_sales <- bss %>%
  filter(year(salesdate) == 2022) %>%
  # Group by quarter and summarize the sales
  group_by(Qtr = quarter) %>%
  summarise(Total_Sales = sum(volume, na.rm = TRUE)) %>%
  ungroup()
sales_ts <- ts(recent_sales$Total_Sales, start = c(2022,1), frequency = 4)
autoplot(sales_ts) + labs(y ="Volume Sold", title = "Quarterly Sales for 2022")

monthly_sales <- bss %>%
  filter(year(salesdate) == 2022) %>%
  # Group by quarter and summarize the sales
  group_by(Month = month) %>%
  summarise(Total_Sales_m = sum(volume, na.rm = TRUE)) %>%
  ungroup()
monthly_sales_ts <- ts(monthly_sales$Total_Sales_m, start = c(2022,1), frequency = 12)
autoplot(monthly_sales_ts) + labs(y ="Volume Sold", title = "Monthly Sales for 2022")


```
<br>
Ok. These graphs weren't very helpful. Maybe we'll need to to check the distribution of the 'comp_x_price' variables.
<br>
We'll come back to it later.
<br>

Before we get into the **Time-Series Analysis**, let's check whether the nature of the seasonality is additive or multiplicative.
<br>
```{r Check the data seasonality, include=TRUE}

bss_season <- bss %>% dplyr::select(salesdate,revenue) %>% group_by(sku) # Create a temporary subset
bss_ts <- ts(bss_season$revenue,frequency = 365) # Capture the yearly seasonality

fit <- stl(bss_ts,s.window = "period")
plot(fit)
additive_fit <- decompose(bss_ts,type = "additive")
multiplicative_fit <- decompose(bss_ts,type = "multiplicative")
plot(bss_ts,ylab = "Sales")

```
<br>
After checking the 'trend' plot, we can confidently say that the seasonality of the plot is additive.
<br>
```{r Basic Time-Series EDA, include=TRUE}

# Do a basic time-series EDA of the sales over the entire period

# Visualize the inherent seasonality 
plot(bss_ts)
plot.ts(bss_ts)

# Check stationarity
adf.test(diff(bss_ts)) 
acf(bss_ts)
# diff(bss_ts)
plot(diff(bss_ts)) # Plot the data after rendering it stationary

```
<br>
After looking at the results of the **Augmented Dickey-Fuller Test** and the **Auto-Correlation Factor** plot, we can confidently say that the data is **stationary**.
<br>
<br>
<br>
```{r Split into training and validation set, include=TRUE}

seed <- 123
set.seed(seed = seed) # Set seed for reproducibility 

bss_train <- bss %>% filter(salesdate <= "2023-05-31") # Create the training set
bss_val <- bss %>% filter(salesdate >= "2023-06-01" & sku == "File Folders SKU 47") # Create the validation set

glimpse(bss_train)

glimpse(bss_val)


```
<br>
```{r Hierarchical Model, include=TRUE}

bss_hierachical_price_product_intercept <- lmer(volume ~ 1 + comp_1_price + comp_2_price + cost + I(price - cost) + salesdate + weekday + (1 + cost | sku), data = bss_train, REML = TRUE)
ranef(bss_hierachical_price_product_intercept)
plot(ranef(bss_hierachical_price_product_intercept))
summary(bss_hierachical_price_product_intercept)

predictions <- predict(bss_hierachical_price_product_intercept, newdata = bss_val)
bss_val$pred_vol <- round(predictions,0)




initial_guess <- median(bss_val$price, na.rm = TRUE)
lower_limit <- min(bss_val$price, na.rm = TRUE)
upper_limit <- max(bss_val$price, na.rm = TRUE)


profit_function <- function(price, model, df) {
predicted_volume <- predict(model, newdata = transform(df, price = price))
profit <- price * predicted_volume - df$cost
return(-sum(profit, na.rm = TRUE))
}


result <- optim(
par = initial_guess,
fn = profit_function,
model = bss_hierachical_price_product_intercept,
df = bss_val,
method = "L-BFGS-B",
lower = lower_limit,
upper = upper_limit
)

t <- round_to_constraint(result$par)
print(paste0("Optimal Price: ", "$",t))

```
<br>
```{r Simulate Model Performance, include=TRUE}

temp <- bss_val %>% filter(salesdate > "2023-08-29" & salesdate <= "2023-09-09")

# Remove the last two rows 
if (nrow(temp) > 2) {
temp <- temp[-((nrow(temp) - 1):(nrow(temp))), ] #Deletes the last two rows
} else {
# Handle the case where you have two or fewer rows
warning("Data frame has two or fewer rows. Cannot remove last two rows.")
}

temp$salesdate <- seq(from = as.Date("2023-09-10"), to = as.Date("2023-09-18"), by = "day")
temp$weekday <- wday(temp$salesdate, label = TRUE) # Introduce a weekday instrumental variable
temp$price <- t
temp$pred_vol <- round(predict(bss_hierachical_price_product_intercept, newdata = temp, re.form = NA),0)
temp$pred_profit <- (temp$price * temp$pred_vol) - temp$cost


p <- mape(temp$profit, temp$pred_profit)
print(paste0("Profit MAPE: ", p, "%"))
```